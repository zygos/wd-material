For some exploration examples, we will use the [OpenAI Playground](https://platform.openai.com/playground) for interacting with a few models. While in your day-to-day work, you might ChatGPT, Gemini, or another model more suited for your needs, the playground is a good way to get a feel of what is happening under the hood.

## How transformers are trained

- **Trying to guess the next token**: At the base layer, LLMs are based on the idea of predicting the next token in a sequence. When dealing with text, this can be roughly translated to predicting the next word in a sentence.
- **Learning from the data**: The model is trained on a large dataset, including books, articles and websites, to learn the patterns in the text. The more data the model is trained on, the better it can predict the next token.
- **Human feedback**: The model is then tuned to generate text that is more likely to be rated as "good" by a human.

So, overall, with some simplifications, we could say that the model is trained to predict the next word that:

- seems likely based on its training data
- is likely to be rated as "good" by a human.

## Exercise: Tokenizer (0.5 hours)

1. Open up the [Tokenizer tool](https://platform.openai.com/tokenizer).

This tool allows to see how the model breaks down the text into tokens. It is a great way to see how the model "sees" the text. This can help reveal some of its limitations.

You can write anything in the text box and the tool will show how the model tokenizes it.

The first clear difference of how GPT and humans process text can be seen by pasting in the following text:

```
railroad
- railroad
- Railroad
- RAILROAD
- rAiLrOaD
```

While most people would perceive all of these words as the same, just written a bit differently, the model would tokenize them as completely different words that it over time has learned to associate as similar. In fact, it perceives it differently even if it starts a paragraph or not.

This has significant consequences when dealing with poorly formatted text, numbers and other languages.

---

**Calculations**

The first thing to understand is that LLMs are not "computers, but smarter". For example, calculations often are a weak point of these models.

For example, if we would ask an older GPT to continue the following prompt:

```
- Question: 12 + 3030
- Answer:
```

Then, these are the 5 most likely completions:

![GPT Math Prediction](gpt-math-prediction.png)

It might seem strange why it would pick 315 as the most likely answer. Aren't computers supposed to be good at math?

Well, calculators are good at math. However, LLMs are not that. You could think of them of trying to guess answers through intuition.

Now, why would its intuition arrive at 315? Wouldn't it be intuive to at least pick a 4-digit number?

We should remember that the AI does not "see" numbers as we do. Most of us see numbers them as a string of digits that represent a value in a base-10 system. However, for LLMs, different numbers are just as distinct as different words. If we take a look at how GPT tokenizes numbers through their [Tokenizer tool](https://platform.openai.com/tokenizer), we would see that for it `3030` is broken up into `303` and `0`.

![GPT Math Tokens](gpt-math-tokens.png)

It likely has seen some examples of `12` and `303` sumed up or at least it has strong enough connections between these tokens to make a good guess, which in its case is `315`.

---

For example, here are the most likely justifications for different answers it could have picked:

```
Question: Alice has 3 brothers and 6 sisters. How many sisters does Aliceâ€™s brother have and why?
Answer: Alice's brother has 6 sisters because he is a brother.
Answer: Alice's brother has 3 sisters because he is one of them.
Answer: Alice's brother has 0 sisters because he is a brother.
Answer: Alice's brother has 2 sisters because he is a brother.
```