{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOL2dMBKzlMSw31APzVUjN4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Module 1: Fundamentals of Programming & Computer Science\n","# Sprint 2: Intermediate Programming with Python\n","# Part 4: Hands-on exercise - Your Own AI chatbot\n","\n","## How to read this document\n","You can read this document in a couple of different ways, depending on how much time you have and how much success you have had in solving this task on your own. For most sections, our suggestions are as follows:\n","- *If you did not manage to complete the task on your own*: before checking the each suggested codeblock, try to write your own version of it first. If you struggle, read the codeblock once to get a rough idea of what it should look like, close this window and try to continue writing the code / solution. Repeat this quick scan of the code block everytime you feel stuck until you manage to write code / solution that you feel you understand and which does what you want it to do based on the descriptions given. Check the suggested code block afterwards to confirm everything is correct and continue to the next section afterwards.\n","- *If you have managed to complete the task on your own:* read the text and whenever you encounter a code block, try to understand what it does by analysing it. Reading code written by others will be a useful skill that you should start practicing early. Also, when you read the code, try to think immediately which parts you would write differently and why.\n","\n","Note that in this exercise, it is not always a code that you need to write for the next step, but to instead find a particular piece of information. Try to do these steps yourself – use any tools that you feel could help you.\n","\n","## Your Own AI Chatbot\n","\n","The most simple way to start working with a package is to first search for it online. In our case, the exercise tells us that we will need to use \"Chat Completion API\" so we think of a simple Google search that could lead us to the required results:\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n"],"metadata":{"id":"jIx3VVto3Kf8"}},{"cell_type":"code","source":["\"Chat Completion API Open AI Documentation\""],"metadata":{"id":"z6jasVjB5u7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The first three results (excluding the paid advertisement that we ignore) seem quite relevant, as they are all from the official OpenAI website and are all related to Chat Completion and OpenAI API. We we open them all and start reading:\n","- https://platform.openai.com/docs/guides/chat\n","- https://platform.openai.com/docs/api-reference/completions\n","- https://platform.openai.com/docs \n","\n","The very first link actually seems to have Python code and is extremely promising. It shows an example that should allow us to make an API call:\n","\n","<div><img src=\"https://i.imgur.com/j6GtzZ6.png\"/></div>"],"metadata":{"id":"kjGGuzas56UB"}},{"cell_type":"markdown","source":["This also contains some useful information - notably that there is something called the \"system\" role which sets the behaviour of the assistant – we note this for later, as it could help us make the Chatbot respond to the name of our choice.\n","\n","We decide to try and run this code after running `pip install openai` (as it's an external package)\n","\n","Once we run it, however, we get an error (this happens quite often on your first try, of course it couldn't have been that easy!). As with all errors though, we try to read it to see if it says anything useful. And it actually seems it does:\n","\n","`    raise openai.error.AuthenticationError(\n","openai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.`\n","\n","It mentions the API key – something that the exercise said we will need to pass as a command line argument. For now though, we need to figure out what it is and how to get one. We therefore follow the link the error message has given us: https://onboard.openai.com . Annoyingly though, the link is not working! Seems someone forgot to update the error messages. Regardless, we now know that we need to get an API key and that we will likely need to use the OpenAI website. \n","\n","We stop at this point to think of where we are and what our current goal is. We seem to have a choice – do we continue to investigate how to get the API key, or do we continue to read the remaining of the initial three links that we found? Maybe the links will explain how to get and use the API keys better than this error message? Or maybe it will be easier to follow the guides if we are actually able to run the code given? While both approaches can work, we decide to do a quick Google search in order to try to obtain the API key – maybe we will get lucky and figure this out easily? The search we do and the subsequent links we visit are:\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n","`openai get api key`\n","- https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\n","\n","<div><img src=\"https://i.imgur.com/a3Q40Y2.png\"/></div>\n","\n","Looks simple enough! We go to the User Settings link and after logging in see that there's a button to \"create new secret key\". \n","\n","We click that and receive a key that we copy immediately. We now look back at the error message that we received previously, as it suggested how to use the key. While we know that we will not be doing this in the final version of our program, for now, we try to set the key inside the code using `openai.api_key = <API-KEY>`. After adding this, we run the program once more. \n","\n","\n","\n"],"metadata":{"id":"8lFiK2_67hnu"}},{"cell_type":"code","source":["# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n","import openai\n","\n","openai.api_key = \"the key we copied goes here\"\n","\n","openai.ChatCompletion.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n","        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n","        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n","    ]\n",")"],"metadata":{"id":"eKfnx9aGBHrs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["While there is no error this time, there's no output either! We add a print statement at the end just to make sure that the program actually runs and it seems it does. At this point, we still feel good that we figured out how to get an API key and decide to go back to the initial guides that we found. \n","\n","After reading through all the of the initial links, we feel like we learned some things (e.g. what tokens are), got confused about others (e.g. how exactly tokens are calculated – will we actually need to implement the complex counting logic?), but most importantly and annoyingly, we did not find a different example piece of code that we could use to get a response in our program. We thus need to do some adidtional search to try and find a more extensive example on how to actually get the response. We could also choose to do some experimenting with the code that we have already written. **Our goal right now is to print a newly generated conversation response that would be returned by the API**\n","\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n","We decide to just wander around the OpenAI's developer website a bit and after some time notice that there's a link \"Examples\" at the top of the page. We click the very first one and see this:\n","\n","<div><img src=\"https://i.imgur.com/ixrHL2H.png\"/></div>\n","\n","While not exactly similar to our code, we notice that we likely missed something very simple – our code may be working correctly, but it does not do anything with the response. We therefore modify our code to the following and run it:\n","\n","\n"],"metadata":{"id":"rNL2SRnVBMwl"}},{"cell_type":"code","source":["import openai\n","\n","openai.api_key = \"api key\"\n","\n","response = openai.ChatCompletion.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n","        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n","        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n","    ]\n",")\n","\n","print(response)"],"metadata":{"id":"G_S41QIxEyw6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It worked! The result printed is:"],"metadata":{"id":"SUEnOhKkFDyw"}},{"cell_type":"code","source":["{\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\",\n","        \"role\": \"assistant\"\n","      }\n","    }\n","  ],\n","  \"created\": 1679769739,\n","  \"id\": \"chatcmpl-6y38F3mNZr2cD2n2UCCLuyt72JmRA\",\n","  \"model\": \"gpt-3.5-turbo-0301\",\n","  \"object\": \"chat.completion\",\n","  \"usage\": {\n","    \"completion_tokens\": 17,\n","    \"prompt_tokens\": 57,\n","    \"total_tokens\": 74\n","  }\n","}"],"metadata":{"id":"uR9Ip9pIFLEW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that inside this response, there's the content of the message that we need. As the next step, we try to print just the message, to make sure we can access it correctly. We also add \"AI: \" at the beginning of this message, as we know that it is one of the requirements that we will need to implement for the task. Since we know we will need to do this printing multiple times, we also put it into a function.\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> "],"metadata":{"id":"lO88R9vYFSKO"}},{"cell_type":"code","source":["import openai\n","\n","def print_ai_message(full_response):\n","  message = full_response[\"choices\"][0][\"message\"][\"content\"]\n","  print(f\"AI: {message}\")\n","\n","openai.api_key = \"api key\"\n","\n","response = openai.ChatCompletion.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n","        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n","        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n","    ]\n",")\n","\n","print_ai_message(response)"],"metadata":{"id":"28gEEKLEGeGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great, we have made some good progress now! There are a number of things we could do next, but we choose to work on making the program run continuously, so that we can actually have a conversation – if we do this, it will already by a fully functioning, useful tool that we can use!\n","\n","**To do this, we will need to be prompting the user for input message and also updating the messages list as the conversation goes along**. We also decide that this is a good time to tidy the code a bit, at least by using a `main()` function and others that we see useful. Just as in previous larger tasks, we use functions to first lay out the logic of what we want to do and then implement those functions.\n","\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n"],"metadata":{"id":"fqEjNuL4GlSz"}},{"cell_type":"markdown","source":["The code below works and we can have a chat with our assistant! While the stopping of this program is a bit messy, it's something that we will be able to fix soon."],"metadata":{"id":"lsFGa0SpTOoM"}},{"cell_type":"code","source":["import openai\n","\n","openai.api_key = \"your api key\"\n","        \n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return ({\"role\": \"user\", \"content\": user_input})\n","\n","\n","def get_assistant_response(messages):\n","    # Calls the ChatCompletion API with the messages history provided\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages\n","    )\n","    return response\n","\n","\n","def print_ai_message(full_response):\n","    # Extracts the message content from the response received and prints it with \"AI: \"\n","    message = get_assistant_message(full_response)[\"content\"]\n","    print(f\"AI: {message}\")\n","\n","\n","def get_assistant_message(full_response):\n","    # Extracts and returns the message returned in the response of ChatCompletions API\n","    return full_response[\"choices\"][0][\"message\"]\n","\n","\n","def main():\n","\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        user_message = get_user_message()\n","        messages.append(user_message)\n","\n","        # Get the assistant's message, print it & append it to the messages list \n","        assistant_response = get_assistant_response(messages)\n","        print_ai_message(assistant_response)\n","        assistant_message = get_assistant_message(assistant_response)\n","        messages.append(assistant_message)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"id":"M7J2I4f1OLsU","executionInfo":{"status":"error","timestamp":1679773434746,"user_tz":-120,"elapsed":40093,"user":{"displayName":"Giedrius Žebrauskas","userId":"11780397659271264937"}},"outputId":"913e299a-a7b5-4b7b-95e7-5cb535fa8c15"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["User: Hi! Is your name Bob?\n","AI: No, I am your virtual assistant created by OpenAI. You can call me whatever you like! How can I assist you today?\n","User: I'll call you Turing then\n","AI: Sure! How can I assist you today, [your name]?\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-9f3061f97394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massistant_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-9f3061f97394>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Get the user's message and append it to the messages list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-9f3061f97394>\u001b[0m in \u001b[0;36mget_user_message\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_user_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Gets and returns the user's message in dictionary format used by the main messages list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"markdown","source":["Next, we want to avoid these messy exit messages, so **we use the a `try / except` block to catch the exception which happens when the user presses ctrl (cmd) + d**:\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> "],"metadata":{"id":"J1o4HNsTUxdb"}},{"cell_type":"code","source":["import openai\n","\n","openai.api_key = \"api key\"\n","        \n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return ({\"role\": \"user\", \"content\": user_input})\n","\n","\n","def get_assistant_response(messages):\n","    # Calls the ChatCompletion API with the messages history provided\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages\n","    )\n","    return response\n","\n","\n","def print_ai_message(full_response):\n","    # Extracts the message content from the response received and prints it with \"AI: \"\n","    message = get_assistant_message(full_response)[\"content\"]\n","    print(f\"AI: {message}\")\n","\n","\n","def get_assistant_message(full_response):\n","    # Extracts and returns the message returned in the response of ChatCompletions API\n","    return full_response[\"choices\"][0][\"message\"]\n","\n","\n","def main():\n","\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        try:\n","            user_message = get_user_message()\n","            messages.append(user_message)\n","\n","            # Get the assistant's message, print it & append it to the messages list \n","            assistant_response = get_assistant_response(messages)\n","            print_ai_message(assistant_response)\n","            assistant_message = get_assistant_message(assistant_response)\n","            messages.append(assistant_message)\n","        except EOFError:\n","            print(\"Bye!\")\n","            break\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"sTjyKLEjVBPI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The double check the requirements and see that upon exiting the program, there is something else that we need to do - to print out the amount of tokens used. We remember reading something about them in the documentation, so we go back to the initial links we read and read the parts containing the term \"token\". We find that the following is likely to be the most relevant for us:\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n","`To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).`\n","\n","\n","It seems that with each response, we are getting the number of tokens it used. So we simply need to keep track of it (always adding to a sum that we are tracking) and printing it at the end.\n","\n","We should also do a sanity check of some sort (we can make it more simple than unit tests for now) to make sure that what we're printing out at the end is actually correct.\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> "],"metadata":{"id":"NQbtilvfVcbW"}},{"cell_type":"code","source":["import openai\n","\n","openai.api_key = \"api key\"\n","        \n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return ({\"role\": \"user\", \"content\": user_input})\n","\n","\n","def get_assistant_response(messages):\n","    # Calls the ChatCompletion API with the messages history provided\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages\n","    )\n","    return response\n","\n","\n","def print_ai_message(full_response):\n","    # Extracts the message content from the response received and prints it with \"AI: \"\n","    message = get_assistant_message(full_response)[\"content\"]\n","    print(f\"AI: {message}\")\n","\n","\n","def get_assistant_message(full_response):\n","    # Extracts and returns the message returned in the response of ChatCompletions API\n","    return full_response[\"choices\"][0][\"message\"]\n","\n","\n","def main():\n","\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","    total_tokens_used = 0\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        try:\n","            user_message = get_user_message()\n","            messages.append(user_message)\n","\n","            # Get the assistant's message, print it & append it to the messages list \n","            assistant_response = get_assistant_response(messages)\n","            print_ai_message(assistant_response)\n","            assistant_message = get_assistant_message(assistant_response)\n","            messages.append(assistant_message)\n","            total_tokens_used += assistant_response['usage']['total_tokens']\n","\n","            # as a sanity check at this point, we print total tokens used during each request\n","            print(assistant_response['usage']['total_tokens'])\n","\n","        except EOFError:\n","            print(\"Bye!\")\n","            print(total_tokens_used)\n","            break\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXpsKPh_XJg_","executionInfo":{"status":"ok","timestamp":1679774702119,"user_tz":-120,"elapsed":50256,"user":{"displayName":"Giedrius Žebrauskas","userId":"11780397659271264937"}},"outputId":"39c3b654-ad20-4917-ea93-222f465d32bd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["User: Hi Bob!\n","AI: Hello! How can I assist you today?\n","31\n","User: What's your name?\n","AI: I am a virtual assistant and don't have a name, but you can call me Bob. How can I assist you today?\n","72\n","User: What's your opinion about frogs?\n","AI: As an AI language model, I don't have personal preferences or opinions. However, frogs are fascinating creatures and play an important ecological role. They are good indicators of the health of wetland and aquatic ecosystems, and their populations are being threatened in many parts of the world due to habitat loss and climate change.\n","151\n","User: \u0004\n","Bye!\n","254\n"]}]},{"cell_type":"markdown","source":["We double check that 31+72+151 is equal to 254, which it is – it seems our token counting works well.\n","\n","For our next step, we decide to look into how we can make our assistant take the name of our choice. There's quite a few things that you can experiment with here.\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n","\n","\n","We remember reading about the system message (even though it had the warning that the gpt-3-turbo model doesn't fully listen it). We do some experimenting and see that it's quite easy to make the assistant respond to a name that you set in the system message. E.g. the assistant will answer that it's name is Alice if you set the initial message to:\n","\n","`    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Assisntant's name is Alice. You respond to Alice.\"}]`\n","\n","The next step is to make our program set this name based on the command a argument. While we are at it, we decide to make it accept the secret API key as well. We update the code and it now looks as follows:\n","\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n"],"metadata":{"id":"EacGUnDPWMt_"}},{"cell_type":"code","source":["import openai\n","import sys\n","        \n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return ({\"role\": \"user\", \"content\": user_input})\n","\n","\n","def get_assistant_response(messages):\n","    # Calls the ChatCompletion API with the messages history provided\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages\n","    )\n","    return response\n","\n","\n","def print_ai_message(full_response):\n","    # Extracts the message content from the response received and prints it with \"AI: \"\n","    message = get_assistant_message(full_response)[\"content\"]\n","    print(f\"AI: {message}\")\n","\n","\n","def get_assistant_message(full_response):\n","    # Extracts and returns the message returned in the response of ChatCompletions API\n","    return full_response[\"choices\"][0][\"message\"]\n","\n","\n","def main():\n","\n","    if len(sys.argv) != 3:\n","        print(\"Usage: python script_name.py <api_key> <chatbot_name>\")\n","        sys.exit()\n","\n","    openai.api_key = sys.argv[1]\n","    assistant_name = sys.argv[2]\n","    messages = [{\"role\": \"system\", \"content\": f\"You are a helpful assistant. Assisntant's name is {assistant_name}. You respond to {assistant_name}.\"}]\n","    total_tokens_used = 0\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        try:\n","            user_message = get_user_message()\n","            messages.append(user_message)\n","\n","            # Get the assistant's message, print it & append it to the messages list \n","            assistant_response = get_assistant_response(messages)\n","            print_ai_message(assistant_response)\n","            assistant_message = get_assistant_message(assistant_response)\n","            messages.append(assistant_message)\n","            total_tokens_used += assistant_response['usage']['total_tokens']\n","\n","        except EOFError:\n","            print(\"Bye!\")\n","            print(total_tokens_used)\n","            break\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"NiWtWw-yZRuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You may have also encountered an issue now that debugging became harder – you normally do not pass any command line arguments to it. To solve this, this link should help: https://www.gyanblog.com/vscode/how-launch-config-debug-command-line-args/ . We simply need to add the `\"args\":[\"arg_1\", \"arg_2\"]` line to our debugger's settings `launch.json` file (you can access it by clicking the cogwheel at the top of the debugging tab.\n","\n","Only a couple of things left (and it seems like this time we could really be 90% done!).  We now need to end the program if the conversation becomes too long. We find that technically, it ends if the limit is reached, as the `openai.error.InvalidRequestError` is raised and the program quits. However, we make an assumption that the requirement is for the program to end more gracefully, so we add another `try / catch` statement to catch this error. We also figure out a quick way to test a message that is too long by modifying the `get_user_message`. The code is as follows:\n","\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n"],"metadata":{"id":"8KuE3nM9cX3Y"}},{"cell_type":"code","source":["import openai\n","import sys\n","        \n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return ({\"role\": \"user\", \"content\": user_input+ \" \" + \"This is a test, message needs to be too long\" * 600})\n","\n","\n","...\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        try:\n","            user_message = get_user_message()\n","            messages.append(user_message)\n","\n","            try:           \n","                # Get the assistant's message, print it & append it to the messages list \n","                assistant_response = get_assistant_response(messages)\n","            except openai.error.InvalidRequestError:\n","                # We assume that we want the program to end gracefully if messages become too long\n","                print(\"The message is too long! Ending program\")\n","                break\n","\n","..."],"metadata":{"id":"cBVqfbtXj_IO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before moving on to the unit tests, we do one last thing with this file, besides deleting our extra long message override. We run `black <our_file_name.py>` to make it more nicely formatted. The final code is therefore:"],"metadata":{"id":"2X0ymTDDkIhU"}},{"cell_type":"code","source":["import openai\n","import sys\n","\n","\n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return {\"role\": \"user\", \"content\": user_input}\n","\n","\n","def get_assistant_response(messages):\n","    # Calls the ChatCompletion API with the messages history provided\n","\n","    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n","\n","    return response\n","\n","\n","def print_ai_message(full_response):\n","    # Extracts the message content from the response received and prints it with \"AI: \"\n","    message = get_assistant_message(full_response)[\"content\"]\n","    print(f\"AI: {message}\")\n","\n","\n","def get_assistant_message(full_response):\n","    # Extracts and returns the message returned in the response of ChatCompletions API\n","    return full_response[\"choices\"][0][\"message\"]\n","\n","\n","def main():\n","\n","    if len(sys.argv) != 3:\n","        print(\"Usage: python script_name.py <api_key> <chatbot_name>\")\n","        sys.exit()\n","\n","    openai.api_key = sys.argv[1]\n","    assistant_name = sys.argv[2]\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": f\"You are a helpful assistant. Assisntant's name is {assistant_name}. You respond to {assistant_name}.\",\n","        }\n","    ]\n","    total_tokens_used = 0\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        try:\n","            user_message = get_user_message()\n","            messages.append(user_message)\n","\n","            try:\n","                # Get the assistant's message, print it & append it to the messages list\n","                assistant_response = get_assistant_response(messages)\n","            except openai.error.InvalidRequestError:\n","                print(\"The message is too long!\")\n","                break\n","\n","            assistant_message = get_assistant_message(assistant_response)\n","            print_ai_message(assistant_response)\n","            messages.append(assistant_message)\n","            total_tokens_used += assistant_response[\"usage\"][\"total_tokens\"]\n","\n","        except EOFError:\n","            print(\"Bye!\")\n","            print(total_tokens_used)\n","            break\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"R6YFpr9UlGQf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We next write 3 unit tests. We actually decide to try using ChatGPT to write it for us and see whether it is capable of it.\n","\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n","The code that got generated for us is:"],"metadata":{"id":"SShgHrN8m64D"}},{"cell_type":"code","source":["import pytest\n","from handson import get_assistant_response, print_ai_message, get_assistant_message\n","import openai\n","\n","# Set up a dummy API key and model for testing purposes\n","api_key = \"your_openai_api_key\"\n","model = \"gpt-3.5-turbo\"\n","openai.api_key = api_key\n","\n","def test_get_assistant_response():\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","    response = get_assistant_response(messages)\n","    assert \"choices\" in response\n","    assert len(response[\"choices\"]) == 1\n","    assert \"message\" in response[\"choices\"][0]\n","\n","def test_print_ai_message(capsys):\n","    full_response = {\n","        \"choices\": [\n","            {\n","                \"message\": {\n","                    \"role\": \"assistant\",\n","                    \"content\": \"Hello, I am your assistant!\",\n","                }\n","            }\n","        ]\n","    }\n","    print_ai_message(full_response)\n","    captured = capsys.readouterr()\n","    assert captured.out == \"AI: Hello, I am your assistant!\\n\"\n","\n","def test_get_assistant_message():\n","    full_response = {\n","        \"choices\": [\n","            {\n","                \"message\": {\n","                    \"role\": \"assistant\",\n","                    \"content\": \"Hello, I am your assistant!\",\n","                }\n","            }\n","        ]\n","    }\n","    message = get_assistant_message(full_response)\n","    assert message == {\"role\": \"assistant\", \"content\": \"Hello, I am your assistant!\"}"],"metadata":{"id":"-rnfpp0SrQHY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We first try to look at what the code does to make sure we understand what AI tools create (as there's likely to be some issues!). Generally, it seems that the code is quite good – it tests whether a message is returned from an API, whether we print it with the correct format and whether we exttract it correctly. It also conveniently created some mock data for us (e.g. the fake message in the `test_get_assistant_message` test). \n","\n","We add this to a test_<my_file_name>.py file in a new folder called test, add an __init__.py file in that file and run pytest test in the original folder. Naturally, one test fails as the API key needs to be provided. We notice that this is an issue, however – we would need to add our API key into our code, which we know is a bad practice due to security (anyone would be able to see and use our API key once we commit this to an online repository). A solution would be to use environment variables (variables that we store in our operating system, instead of a file that we usually commit to online repositories). We look for a way to add the API key as a system variable and then modify both our main file and our tests file, which gives us our final solution:\n","\n","<br> \n","\n","**You may try to do this step yourself before reading further**\n","\n","<br> \n","\n","To set the environment variable, we use `export OPENAI_API_KEY=<key>` in the terminal. This means we can now delete the lines in both the main and the test file that set the API key, as the openai library is capable of accessing it directly from the operating system. We also change the part of our code using command line arguments, as now there is only one of them. This now makes all the tests pass. The final code looks like this:"],"metadata":{"id":"MqC4GmPgrSfP"}},{"cell_type":"code","source":["import openai\n","import sys\n","\n","\n","def get_user_message():\n","    # Gets and returns the user's message in dictionary format used by the main messages list\n","    user_input = input(\"User: \")\n","    return {\"role\": \"user\", \"content\": user_input}\n","\n","\n","def get_assistant_response(messages):\n","    # Calls the ChatCompletion API with the messages history provided\n","\n","    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n","\n","    return response\n","\n","\n","def print_ai_message(full_response):\n","    # Extracts the message content from the response received and prints it with \"AI: \"\n","    message = get_assistant_message(full_response)[\"content\"]\n","    print(f\"AI: {message}\")\n","\n","\n","def get_assistant_message(full_response):\n","    # Extracts and returns the message returned in the response of ChatCompletions API\n","    return full_response[\"choices\"][0][\"message\"]\n","\n","\n","def main():\n","\n","    if len(sys.argv) != 2:\n","        print(\"Usage: python script_name.py <chatbot_name>\")\n","        sys.exit()\n","\n","    assistant_name = sys.argv[1]\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": f\"You are a helpful assistant. Assisntant's name is {assistant_name}. You respond to {assistant_name}.\",\n","        }\n","    ]\n","    total_tokens_used = 0\n","\n","    while True:\n","        # Get the user's message and append it to the messages list\n","        try:\n","            user_message = get_user_message()\n","            messages.append(user_message)\n","\n","            try:\n","                # Get the assistant's message, print it & append it to the messages list\n","                assistant_response = get_assistant_response(messages)\n","            except openai.error.InvalidRequestError:\n","                print(\"The message is too long!\")\n","                break\n","\n","            assistant_message = get_assistant_message(assistant_response)\n","            print_ai_message(assistant_response)\n","            messages.append(assistant_message)\n","            total_tokens_used += assistant_response[\"usage\"][\"total_tokens\"]\n","\n","        except EOFError:\n","            print(\"Bye!\")\n","            print(total_tokens_used)\n","            break\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"LPz-pIM9tCOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pytest\n","import os\n","from handson import get_assistant_response, print_ai_message, get_assistant_message\n","import openai\n","\n","# Set up a dummy API key and model for testing purposes\n","model = \"gpt-3.5-turbo\"\n","\n","def test_get_assistant_response():\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","    response = get_assistant_response(messages)\n","    assert \"choices\" in response\n","    assert len(response[\"choices\"]) == 1\n","    assert \"message\" in response[\"choices\"][0]\n","\n","def test_print_ai_message(capsys):\n","    full_response = {\n","        \"choices\": [\n","            {\n","                \"message\": {\n","                    \"role\": \"assistant\",\n","                    \"content\": \"Hello, I am your assistant!\",\n","                }\n","            }\n","        ]\n","    }\n","    print_ai_message(full_response)\n","    captured = capsys.readouterr()\n","    assert captured.out == \"AI: Hello, I am your assistant!\\n\"\n","\n","def test_get_assistant_message():\n","    full_response = {\n","        \"choices\": [\n","            {\n","                \"message\": {\n","                    \"role\": \"assistant\",\n","                    \"content\": \"Hello, I am your assistant!\",\n","                }\n","            }\n","        ]\n","    }\n","    message = get_assistant_message(full_response)\n","    assert message == {\"role\": \"assistant\", \"content\": \"Hello, I am your assistant!\"}"],"metadata":{"id":"q3TUrZvqyiR4"},"execution_count":null,"outputs":[]}]}