1. What is an embedding?

a) Any type of vector.
b) A database index for semantic search.
c) A programming language designed for AI.
d) A numerical representation of text or other data.
- Correct: d) A numerical representation of text or other data.
- Topic: Embeddings
- Difficulty: Basic
- Resource: https://txt.cohere.com/sentence-word-embeddings/

2. True or False: Embeddings can only be used to represent text.
a) True
b) False
Correct: b) False
Topic: Embeddings
Difficulty: Basic
Resource: https://txt.cohere.com/sentence-word-embeddings/

3. To maximize the effectiveness of your RAG chatbot, when should you consider recomputing embeddings for your contextual documents?
a) Every time the chatbot is queried, for maximum up-to-date information.
b) After updates are made to the documents.
c) Only if the embedding model itself has been updated or changed.
d) Recomputing embeddings is unnecessary.
Correct: b) After updates are made to the documents.
Topic: Embeddings
Difficulty: Basic
Resource: https://txt.cohere.com/sentence-word-embeddings/

4. What is a primary use case for vector databases in the context of chatbots?
a) To store chatbot conversations for later analysis
b) To secure sensitive data related to users conversations
c) To store and execute chatbot logic
d) To store and retrieve embeddings for documents and compute their similarity to a given query
- Correct: d) To store and retrieve embeddings for documents and compute their similarity to a given query
- Topic: Vector Databases
- Difficulty: Basic
- Resource: [What is a Vector Database?](https://www.youtube.com/watch?v=t9IDoenf-lo)

5. When implementing a chatbot with RAG, is there a scenario where you can use one service for dealing with embeddings and another to generate responses?
a) Yes.
b) Yes, as long as both services use the same model.
c) Yes, but the responses will be less accurate.
d) No, embeddings and generating responses must be handled by the same service.
- Correct: a) Yes.
- Topic: Embeddings
- Difficulty: Advanced
- Resource: [RAG Chatbot](https://docs.google.com/document/d/1vhMAuYBhC0j5bLQa4L6OwEJo5gOLdf5ywUCzKcT6ddA/edit#heading=h.9l6gn15l1qm)

6. In a Retrieval Augmented Generation (RAG) system, what is the role of the vector database?
a) Generates the final response to the user.
b) Stores and retrieves relevant documents based on the user's query.
c) Calculates embeddings for the user's query.
d) Handles natural language understanding tasks.
- Correct: b) Stores and retrieves relevant documents based on the user's query.
- Topic: Retrieval Augmented Generation
- Difficulty: Basic
- Resource: https://txt.cohere.com/exploring-chat-rag/

7. You are dealing with an embedding service that uses a powerful model which "understands" lots of concepts, expressions, idioms and supports various languages. If you have a phrase "apple tree" expressed as an embedding, which of the following phrases is most likely to have a similar embedding, measured by dot product/cosine similarity? The correct answer is significantly closer to "apple tree" than the other options. Results can be validated with OpenAI's text-embedding-3-large.
a) "easy as apple pie", because it contains the word "apple".
b) "apfelbaum" (German for "apple tree"), because it expresses the same concept.
c) "helicopter", because it refers to an object in English.
d) "dendrology", because it is a scientific field related to trees.
- Correct: b) "apfelbaum" (German for "apple tree"), because it expresses the same concept.
- Topic: Embeddings
- Difficulty: Advanced
- Resource:

8. Which of the following is NOT a typical use case for a vector database?
a) Personalized recommendations.
b) Real-time search based on semantic similarity.
c) Managing inventory in an e-commerce system.
d) Embeddings for documents.
- Correct: c) Managing inventory in an e-commerce system.
- Topic: Vector Databases
- Difficulty: Basic
- Resource: [What is a Vector Database?](https://www.youtube.com/watch?v=t9IDoenf-lo)

9. You are working with text embeddings of 128 dimensions. What does this dimensionality represent?
a) The total number of words in the text that was embedded
b) The maximum length of text that can be embedded
c) The relative importance of each word in the text
d) The number of features the embedding model uses to represent the text
Correct: d) The number of features the embedding model uses to represent the text
Topic: Embeddings
Difficulty: Basic
Resource: https://txt.cohere.com/sentence-word-embeddings/

10. When using cosine similarity to compare embeddings, what would a raw similarity value of 99 indicate?
a) The two pieces of text are very similar in meaning
b) The two pieces of text are completely different in meaning
c) The two pieces of text have roughly the same word count
d) There is a mistake in cosine similarity calculation
Correct: d) There is a mistake in cosine similarity calculation
Topic: Embeddings
Difficulty: Advanced
Resource: https://txt.cohere.com/what-is-similarity-between-sentences/
